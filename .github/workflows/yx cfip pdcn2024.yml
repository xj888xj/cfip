name: YX CFIP and DNS pdcn2024

on:
  schedule:
    - cron: '10 */4 * * *' # 每4小时运行一次
  workflow_dispatch: # 手动触发

jobs:
  run-and-commit:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4

    - name: Run the yx_ips script
      env:
        CF_API_KEY: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        CF_ZONE_ID: ${{ secrets.CLOUDFLARE_ZONE_ID }}
        CF_DOMAIN_NAME: ${{ secrets.CLOUDFLARE_SUBDOMAIN }}
        CF_API_EMAIL: ${{ secrets.CLOUDFLARE_EMAIL }}
      run: |
        import os
        import requests
        from bs4 import BeautifulSoup
        import re

        # Cloudflare API配置信息
        CF_API_KEY = os.getenv('CF_API_KEY')
        CF_ZONE_ID = os.getenv('CF_ZONE_ID')
        CF_DOMAIN_NAME = os.getenv('CF_DOMAIN_NAME')
        CF_API_EMAIL = os.getenv('CF_API_EMAIL')

        # 定义请求头
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        # 定义五个网址
        urls = [
            "https://cf.090227.xyz/",
            "https://stock.hostmonit.com/CloudFlareYes",
            "https://ip.164746.xyz/",
            "https://monitor.gacjie.cn/page/cloudflare/ipv4.html",
            "https://www.182682.xyz/page/cloudflare/ipv4.html"
        ]

        # 解析延迟数据的正则表达式
        latency_pattern = re.compile(r'(\d+(\.\d+)?)\s*(ms|毫秒)?')

        # 提取表格数据的函数
        def extract_table_data(url):
            try:
                response = requests.get(url, headers=headers)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    return soup
                else:
                    print(f"Failed to fetch data from {url}. Status code: {response.status_code}")
            except requests.RequestException as e:
                print(f"Request failed for {url}: {e}")
            return None

        # 处理每个网址的数据
        def process_site_data(url):
            soup = extract_table_data(url)
            if not soup:
                return []

            data = []
            if "cf.090227.xyz" in url:
                rows = soup.find_all('tr')
                for row in rows:
                    columns = row.find_all('td')
                    if len(columns) >= 3:
                        line_name = columns[0].text.strip()
                        ip_address = columns[1].text.strip()
                        latency_text = columns[2].text.strip()
                        latency_match = latency_pattern.match(latency_text)
                        if latency_match:
                            latency_value = latency_match.group(1)
                            latency_unit = 'ms'
                            data.append(f"{ip_address}#{line_name}-{latency_value}{latency_unit}")

            elif "stock.hostmonit.com" in url:
                rows = soup.find_all('tr', class_=re.compile(r'el-table__row'))
                for row in rows:
                    columns = row.find_all('td')
                    if len(columns) >= 3:
                        line_name = columns[0].text.strip()
                        ip_address = columns[1].text.strip()
                        latency_text = columns[2].text.strip()
                        latency_match = latency_pattern.match(latency_text)
                        if latency_match:
                            latency_value = latency_match.group(1)
                            latency_unit = 'ms'
                            data.append(f"{ip_address}#{line_name}-{latency_value}{latency_unit}")

            elif "ip.164746.xyz" in url:
                rows = soup.find_all('tr')
                for row in rows:
                    columns = row.find_all('td')
                    if len(columns) >= 5:
                        ip_address = columns[0].text.strip()
                        latency_text = columns[4].text.strip()
                        latency_match = latency_pattern.match(latency_text)
                        if latency_match:
                            latency_value = latency_match.group(1)
                            latency_unit = 'ms'
                            data.append(f"{ip_address}-{latency_value}{latency_unit}")

            elif "monitor.gacjie.cn" in url:
                rows = soup.find_all('tr')
                for row in rows:
                    tds = row.find_all('td')
                    if len(tds) >= 5:
                        line_name = tds[0].text.strip()
                        ip_address = tds[1].text.strip()
                        latency_text = tds[4].text.strip()
                        latency_match = latency_pattern.match(latency_text)
                        if latency_match:
                            latency_value = latency_match.group(1)
                            latency_unit = 'ms'
                            data.append(f"{ip_address}#{line_name}-{latency_value}{latency_unit}")

            elif "www.182682.xyz" in url:
                rows = soup.find_all('tr', class_=re.compile(r'line-cm|line-ct|line-cu'))
                for row in rows:
                    tds = row.find_all('td')
                    if len(tds) >= 4:
                        line_name = tds[0].text.strip()
                        ip_address = tds[1].text.strip()
                        latency_text = tds[3].text.strip()
                        latency_match = latency_pattern.match(latency_text)
                        if latency_match:
                            latency_value = latency_match.group(1)
                            latency_unit = 'ms'
                            data.append(f"{ip_address}#{line_name}-{latency_value}{latency_unit}")

            return data

        # 主函数，处理所有网站的数据
        def main():
            all_data = []
            for url in urls:
                site_data = process_site_data(url)
                all_data.extend(site_data)

            # 去除重复的IP地址行
            unique_data = list(set(all_data))

            # 过滤延迟数据低于100ms的行
            filtered_data = [line for line in unique_data if float(line.split('-')[-1].replace('ms', '')) < 100]

            # 写入到yx_ips.txt文件
            with open('yx_ips.txt', 'w', encoding='utf-8') as f:
                for line in filtered_data:
                    f.write(line + '\n')

            # 执行清空DNS记录的操作
            clear_dns_records()

            # 从yx_ips.txt文件中提取IPv4地址
            with open("yx_ips.txt", "r") as file:
                ipv4 = [line.split('#')[0] for line in file if '#' in line]

            # 执行添加DNS记录的操作
            for ip in ipv4:
                add_dns_record(ip)

        # 清空CF_DOMAIN_NAME的所有DNS记录
        def clear_dns_records():
            url = f"https://api.cloudflare.com/client/v4/zones/{CF_ZONE_ID}/dns_records?name={CF_DOMAIN_NAME}"
            headers = {
                "Authorization": f"Bearer {CF_API_KEY}",
                "X-Auth-Email": CF_API_EMAIL,
                "Content-Type": "application/json"
            }

            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                records = response.json().get('result', [])
                for record in records:
                    delete_url = f"https://api.cloudflare.com/client/v4/zones/{CF_ZONE_ID}/dns_records/{record['id']}"
                    delete_response = requests.delete(delete_url, headers=headers)
                    if delete_response.status_code == 200:
                        print(f"Successfully deleted DNS record: {record['id']}")
                    else:
                        print(f"Failed to delete DNS record: {record['id']}, status code: {delete_response.status_code}, response: {delete_response.text}")
            else:
                print(f"Failed to fetch DNS records, status code: {response.status_code}, response: {response.text}")

        # 添加新的IPv4地址为DNS记录，在 add_dns_record 函数中添加调试输出
        def add_dns_record(ip):
            # 重新获取 ipv4 地址
            with open("yx_ips.txt", "r") as file:
                ipv4 = [line.split('#')[0] for line in file if '#' in line]

            print(f"Adding DNS record for IP: {ip}")
            url = f"https://api.cloudflare.com/client/v4/zones/{CF_ZONE_ID}/dns_records"
            headers = {
                "Authorization": f"Bearer {CF_API_KEY}",
                "X-Auth-Email": CF_API_EMAIL,
                "Content-Type": "application/json"
            }
            data = {
                "type": "A",
                "name": CF_DOMAIN_NAME,
                "content": ip,
                "ttl": 60,  # 设置TTL为1分钟
                "proxied": False
            }
            response = requests.post(url, headers=headers, json=data)
            if response.status_code == 200:
                print(f"Successfully created DNS record for IP: {ip}")
            else:
                print(f"Failed to create DNS record for IP: {ip}, status code: {response.status_code}, response: {response.text}")

        if __name__ == "__main__":
            main()

    - name: Commit and push changes
      run: |
        git config --global user.email "actions@github.com"
        git config --global user.name "GitHub Actions"
        git add yx_ips.txt
        git commit -m "Update yx_ips.txt" || echo "No changes to commit"
        git push origin main